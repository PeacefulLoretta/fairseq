# filepath: C:\Users\law26\Desktop\VoiceCloningProject\fairseq\fairseq\config\config.yaml
# @package _group_

hydra:  
  run:
    dir: .

defaults:
  - _self_
  - task: null
  - model: null
  - criterion: cross_entropy
  - optimizer: null
  - lr_scheduler: fixed
  - bpe: null
  - tokenizer: null
  - scoring: null
  - generation: null
  - common_eval: null
  - eval_lm: null

common:
  no_progress_bar: False
  log_interval: 1
  log_format: 'json'
  log_file: null
  aim_repo: null
  aim_run_hash: null
  tensorboard_logdir: ''
  wandb_project: null
  azureml_logging: False
  seed: 1337
  cpu: False
  tpu: False
  bf16: False
  memory_efficient_bf16: False
  fp16: True
  memory_efficient_fp16: False
  fp16_no_flatten_grads: False
  fp16_init_scale: 128
  fp16_scale_window: null
  fp16_scale_tolerance: 0.0
  on_cpu_convert_precision: False
  min_loss_scale: 0.0001
  threshold_loss_scale: null
  amp: False
  amp_batch_retries: 2
  amp_init_scale: 128
  amp_scale_window: null
  user_dir: null
  empty_cache_freq: 0
  all_gather_list_size: 16384
  model_parallel_size: 1
  quantization_config_path: null
  profile: False
  reset_logging: False
  suppress_crashes: False
  use_plasma_view: False
  plasma_path: '/tmp/plasma'

common_eval:
  path: null
  post_process: null
  quiet: False
  model_overrides: '{}'
  results_path: null

dataset:
  num_workers: 6
  skip_invalid_size_inputs_valid_test: True
  max_tokens: 1400000
  batch_size: null
  required_batch_size_multiple: 8
  required_seq_len_multiple: 1
  dataset_impl: null
  data_buffer_size: 10
  train_subset: 'train'
  valid_subset: 'valid'
  combine_valid_subsets: null
  ignore_unused_valid_subsets: False
  validate_interval: 5
  validate_interval_updates: 10000
  validate_after_updates: 0
  fixed_validation_seed: null
  disable_validation: False
  max_tokens_valid: 1400000
  batch_size_valid: '${dataset.batch_size}'
  max_valid_steps: null
  curriculum: 0
  gen_subset: 'test'
  num_shards: 1
  shard_id: 0
  grouped_shuffling: False
  update_epoch_batch_itr: True
  update_ordered_indices_seed: False

distributed_training:
  distributed_world_size: 32
  distributed_num_procs: 1
  distributed_rank: 0
  distributed_backend: 'nccl'
  distributed_init_method: 'tcp://learnfair1408:55498'
  distributed_port: 55498
  device_id: 0
  distributed_no_spawn: True
  ddp_backend: 'no_c10d'
  ddp_comm_hook: 'none'
  bucket_cap_mb: 25
  fix_batches_to_gpus: False
  find_unused_parameters: True
  gradient_as_bucket_view: False
  fast_stat_sync: False
  heartbeat_timeout: -1
  broadcast_buffers: False
  slowmo_momentum: null
  slowmo_base_algorithm: 'localsgd'
  localsgd_frequency: 3
  nprocs_per_node: 8
  pipeline_model_parallel: False
  pipeline_balance: null
  pipeline_devices: null
  pipeline_chunks: 0
  pipeline_encoder_balance: null
  pipeline_encoder_devices: null
  pipeline_decoder_balance: null
  pipeline_decoder_devices: null
  pipeline_checkpoint: 'never'
  zero_sharding: 'none'
  fp16: True
  memory_efficient_fp16: False
  tpu: False
  no_reshard_after_forward: False
  fp32_reduce_scatter: False
  cpu_offload: False
  use_sharded_state: False
  not_fsdp_flatten_parameters: False

optimization:
  max_epoch: 0
  max_update: 400000
  stop_time_hours: 0.0
  clip_norm: 10.0
  sentence_avg: False
  update_freq: [1]
  lr: [0.0005]
  stop_min_lr: -1.0
  use_bmuf: False
  skip_remainder_batch: False
  debug_param_names: False